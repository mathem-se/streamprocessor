{
  "image": "ARTIFACT_REGISTRY",
  "name": "Backfill",
  "defaultEnvironment": {},
  "sdk_info": {
    "language": "JAVA"
  },
  "metadata": {
    "description": "An Apache Beam streaming pipeline that reads serialized backup payloads into corresponding BigQuery table",
    "parameters": [
      {
        "name": "backfillQuery",
        "label": "Backfill Query",
        "helpText": "The query that is to extract from backfill table. Target table is defined in each row's payload.",
        "isOptional": false
      },
      {
        "name": "pipelineType",
        "label": "Pipeline type",
        "helpText": "We apply logic depending on the source provider och type of data",
        "isOptional": false,
        "regexes": [
          "[_a-zA-Z]+"
        ]
      },
      {
        "name": "firestoreProjectId",
        "label": "Firestore Project Id",
        "helpText": "GCP Firestore Project Id, if other than the project where the dataflow runs.",
        "isOptional": false,
        "regexes": [
          "[-_\/a-zA-Z0-9]+"
        ]
      },
      {
        "name": "deadLetterTopic",
        "label": "Pubsub deadletter topic.",
        "helpText": "GCP Pubsub topic to write deadletter records to, example projects/jhfskdh/topics/test",
        "isOptional": true,
        "regexes": [
          "[-_\/a-z0-9]+"
        ]
      },
      {
        "name": "numberOfWorkerHarnessThreads",
        "label": "Number of worker harness threads",
        "helpText": "The number of threads per each worker harness process (default: 300)",
        "isOptional": true,
        "regexes": [
          "[0-9]+"
        ]
      },
      {
        "name": "diskSizeGb",
        "label": "Disk size in GB for dataflow workers",
        "helpText": "The disk size in GB, use larger disk size for large backfills",
        "isOptional": false,
        "regexes": [
          "[0-9]+"
        ]
      },
      {
        "name": "schemaCheckRatio",
        "label": "Ratio between 0-1 to check for unmapped fields in message.",
        "helpText": "0.01 = default. 0 = no checks.",
        "isOptional": true,
        "regexes": [
          "^([0-9]+([.][0-9]*)?|[.][0-9]+)$"
        ]
      },
      {
        "name": "dataContractsServiceUrl",
        "label": "Data contract service URL",
        "helpText": "Base url for data contract API. API has to have a /contract/topic endpoint.",
        "isOptional": false,
        "regexes": ["https?:\/\/.*"]
      },
      {
        "name": "numStorageWriteApiStreams",
        "label": "Number of storage write streams",
        "helpText": "Number of storage write streams. > 0",
        "isOptional": true,
        "regexes": [
          "[0-9]+"
        ]
      },
      {
        "name": "storageWriteApiTriggeringFrequencySec",
        "label": "BigQuery Storage Write Triggering Frequency",
        "helpText": "BigQuery triggering frequency (seconds).",
        "isOptional": true,
        "regexes": [
          "[0-9]+"
        ]
      }
    ]
  }
}